{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a81642c1-6977-4637-b2d5-802f4a3255c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4eb678-2d02-41a0-981d-c6ab8204a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = \"AIzaSyBUVvgM4XgmGOXeHu-FKsJqehdCrrXW_pY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf7655c5-4bde-413d-a877-5c443b5d316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf6307fe-cb6c-4bb0-9068-ba226ef457c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ArticleSummaryOutput(BaseModel):\n",
    "    \n",
    "    article_url: str = Field(..., description=\"The Substack article URL\")\n",
    "    summary: str = Field(..., description=\"Short, meaningful summary of the article\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f9304d1-d259-4648-9d9d-a4442cc19e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_url = \"https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c9e6989f-c334-496c-b51a-d4dfcfb8519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article_url: str, client: genai.Client) -> ArticleSummaryOutput:\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the article at this URL in 2–3 concise sentences.\n",
    "    Return a JSON with:\n",
    "    - article_url: repeat the URL\n",
    "    - summary: the main idea in 2–3 sentences\n",
    "\n",
    "    URL: {article_url}\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[prompt],\n",
    "        config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_json_schema\": ArticleSummaryOutput.model_json_schema(),\n",
    "        },\n",
    "    )\n",
    "    return ArticleSummaryOutput.model_validate_json(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d86dbfdc-b5b1-407c-80e2-18bccef79709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleSummaryOutput(article_url='https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated', summary=\"Gary Marcus discusses how OpenAI's O3 and Grok-4, despite not being explicitly designed with symbolic AI in mind, exhibit improved reasoning abilities that inadvertently validate the importance of integrating symbolic approaches with neural networks. He argues that their unexpected success suggests a convergence towards hybrid AI systems that combine the strengths of both connectionist and symbolic methods for more robust and reliable AI.\")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_result = summarize_article(article_url, client)\n",
    "summary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "df75fc50-73a1-4425-8ace-1f0239312bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ArticleQuotedInfoOutput(BaseModel):\n",
    "    article_url: str = Field(..., description=\"URL of the article\")\n",
    "    \n",
    "    quoted_information: list[str] = Field(..., description=\"List of quotes or quoted phrases found in the article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cde223f0-8d92-427c-9e7d-fa2db8166f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ArticleQuotedInfoOutput(BaseModel):\n",
    "    article_url: str = Field(..., description=\"URL of the article\")\n",
    "    \n",
    "    quoted_information: list[str] = Field(..., description=\"List of quotes or quoted phrases found in the article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e7118953-b639-4d51-bdd0-935f95e64263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article_with_quotes(article_url: str, client: genai.Client) -> ArticleQuotedInfoOutput:\n",
    "    prompt = f\"\"\"\n",
    "    Read the article at the URL below and return structured JSON with:\n",
    "    - \"article_url\": repeat the input URL\n",
    "    \n",
    "    - \"quoted_information\": a list of all phrases or sentences that appear within quotes in the article (e.g. “like this” or 'like that')\n",
    "\n",
    "    Only respond with valid JSON in the correct schema.\n",
    "\n",
    "    URL: {article_url}\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[prompt],\n",
    "        config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_json_schema\": ArticleQuotedInfoOutput.model_json_schema(),\n",
    "        },\n",
    "    )\n",
    "    return ArticleQuotedInfoOutput.model_validate_json(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "87f6c9fc-c0c9-4bb7-9294-118638236f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleQuotedInfoOutput(article_url='https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated', quoted_information=['“the ability to retain facts over long contexts,”', '“needle in a haystack”', '“long context recall”', '“We believe that this demonstrates that we can significantly improve long context recall without sacrificing quality in other areas.”', '“near-perfect recall”', '“These results suggest that Grok-1.5 exhibits near-perfect recall up to 128K tokens in our evaluations.”', '“near-perfect recall”', '“can’t handle the long tail”', '“In other words, when models are evaluated for their ability to retrieve relevant information from longer contexts, performance drops off significantly. This is what we mean by can’t handle the long tail.”', '“the AI community (or at least a vocal subset thereof) has been consistently overconfident”', '“delusional levels”', '“an uncanny ability to retrieve information from across a 128,000 token context window,”', '“the AI community (or at least a vocal subset thereof) has been consistently overconfident”', '“delusional levels”', '“hallucinating less”', '“a big deal”', '“long context is all you need”', '“System 1”', '“System 2”', '“System 1”', '“System 2”', '“System 1”', '“System 2”', '“System 1”', '“System 2”'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quoted_result = summarize_article_with_quotes(article_url, client)\n",
    "\n",
    "quoted_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "af793bd1-5494-46ee-95a4-b9e02113275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_info = quoted_result.quoted_information\n",
    "url = quoted_result.article_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1e7bd253-295b-46eb-b40a-9aec38531d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 {\n",
      "  \"article_url\": \"https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated\",\n",
      "  \"summary\": \"Gary Marcus discusses how OpenAI's O3 and Grok-1.5, while still flawed, demonstrate a move towards more reliable and less hallucinatory AI models. He argues that their improved performance, particularly in reasoning and coherence, inadvertently supports the need for incorporating more structured approaches and 'deep understanding' into AI development, vindicating his long-held views.\"\n",
      "}\n",
      "\n",
      "🔹 “the ability to retain facts over long contexts,”\n",
      "🔹 “needle in a haystack”\n",
      "🔹 “long context recall”\n",
      "\n",
      "Read the full article 👉 https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated\n",
      "#AI #TechNews #LinkedInLearning\n"
     ]
    }
   ],
   "source": [
    "def generate_linkedin_post(summary: str, quoted_information: list[str], article_url: str) -> str:\n",
    "    bullets = \"\\n\".join([f'🔹 {quote}' for quote in quoted_information[:3]])\n",
    "    linkedin_post = f\"\"\"\n",
    "🚀 {summary.strip()}\n",
    "\n",
    "{bullets}\n",
    "\n",
    "Read the full article 👉 {article_url}\n",
    "#AI #TechNews #LinkedInLearning\n",
    "\"\"\".strip()\n",
    "    return linkedin_post\n",
    "\n",
    "# Generate post\n",
    "linkedin_post = generate_linkedin_post(summary, quoted_info , article_url)\n",
    "print(linkedin_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5a6b5cc9-5d81-4dfa-a20a-f4d340969da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Key Highlights from the Article:*\n",
      "🔹 “the ability to retain facts over long contexts,”\n",
      "🔹 “needle in a haystack”\n",
      "🔹 “long context recall”\n",
      "🔹 “We believe that this demonstrates that we can significantly improve long context recall without sacrificing quality in other areas.”\n",
      "🔹 “near-perfect recall”\n"
     ]
    }
   ],
   "source": [
    "def create_whatsapp_post(quoted_information: list[str], article_url: str) -> str:\n",
    "    bullets = \"\\n\".join([f'🔹 {quote}' for quote in quoted_information[:5]])\n",
    "    whatsapp_post = f\"\"\"\n",
    "  *Key Highlights from the Article:*\n",
    "{bullets}\n",
    "\n",
    " \n",
    "\"\"\".strip()\n",
    "    return whatsapp_post\n",
    "\n",
    "# Example usage\n",
    "whatsapp_post = create_whatsapp_post(quoted_info, article_url)\n",
    "print(whatsapp_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "36d50612-b4c0-4d02-9f5b-2c57555d3109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 \"“the ability to retain facts over long contexts,”\"\n",
      "💬 \"“needle in a haystack”\"\n",
      "\n",
      "🔗 https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated\n",
      "#AI #News\n"
     ]
    }
   ],
   "source": [
    "def create_twitter_post(quoted_information: list[str], article_url: str) -> str:\n",
    "    tweet_lines = [f'💬 \"{quote}\"' for quote in quoted_information[:2]]  # Max 2 quotes for brevity\n",
    "    quotes_section = \"\\n\".join(tweet_lines)\n",
    "    \n",
    "    hashtags = \"#AI #News\"  \n",
    "    \n",
    "    twitter_post = f\"\"\"\n",
    "{quotes_section}\n",
    "\n",
    "🔗 {article_url}\n",
    "{hashtags}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    \n",
    "    if len(twitter_post) > 280:\n",
    "        # Trim quotes if necessary\n",
    "        trimmed_quotes = [q[:100] + \"...\" if len(q) > 100 else q for q in quoted_information[:2]]\n",
    "        quotes_section = \"\\n\".join([f'💬 \"{quote}\"' for quote in trimmed_quotes])\n",
    "        twitter_post = f\"{quotes_section}\\n\\n🔗 {article_url}\\n{hashtags}\"\n",
    "\n",
    "    return twitter_post\n",
    "\n",
    "\n",
    "twitter_post = create_twitter_post(quoted_info, article_url)\n",
    "print(twitter_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce132cb-94d3-4062-832f-5acf44726132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea469ff-0e9d-4183-aab0-75f6331ee913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
